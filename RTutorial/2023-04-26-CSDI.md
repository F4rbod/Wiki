---
title: Notes on Time Series Imputation Using Score-Based (Diffusion) Models
description: 
published: true
date: 2023-05-14T17:08:35.106Z
tags: 
editor: markdown
dateCreated: 2023-05-14T14:57:19.299Z
---

# Introduction

## Time Series Data

Time series data is encountered in many fields, including medicine, finance, and meteorology. Missing data is a common problem in time series data. For example, in medical data, a patient may miss a scheduled visit, or a patient may not be able to complete a questionnaire. In finance, a stock may not be traded on a given day. In meteorology, a weather station may be down for maintenance. In all of these cases, the missingness may follow different patterns, and current methods for the imputation of missing data are not always satisfactory.
Overall, we can classify missing data imputation methods into two categories: (1) **Deterministic Imputation** and (2) **Probabilistic Imputation**. The earlier resulting in a point estimate of the missing value, and the latter resulting in a distribution over the missing value. Although we can use the mean or median of the estimated distribution as a point estimate, the distribution itself can be used to quantify the uncertainty in the imputed value. In this post, we will focus on the use of Score-based (Diffusion) models for probabilsitc imputation of missing data in time series.

## Time Series Imputation

Some of the deep learning approaches to deterministic time series imputation include:
- Recurrent Neural Networks (RNNs): these use a recurrent structure to model the temporal dependencies in the data. The most popular RNNs are the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks.
- Generative Adversarial Networks (GANs): these use a generator network to generate the missing data and a discriminator network to distinguish between the generated and real data. The generator and discriminator are trained in an adversarial manner, where the generator tries to fool the discriminator, and the discriminator tries to distinguish between the generated and real data.
- RNNs with attention: these use an attention mechanism to focus on the relevant parts of the data. The attention mechanism is typically used in conjunction with an RNN.

Probabilistic time series imputation methods include:
- Gaussian Process Variation Autoencodoers (GP-VAEs): these are one of the methods used for probabilistic time series imputation. In this method, the high-dimensional time series are represented in a lower-dimensional space which evolves smoothly in time according to a Gaussian Process. 
- Score-Based (Diffusion) Models: this is the method we will focus on in this post. Briefly, they work by adding random noise across many steps to the data so that the data becomes nearly a standard normal distribution. The model is then trained to reverse this process so that we have a method of going from a standard normal distribution to the original data.

# Score-Based (Diffusion) Models

The main objective of generative models is to, given a dataset $X = \{x_1, x_2, \ldots, x_N\}$, to learn a distribution $p_\theta(x)$ that can generate data similar to the data in $X$.

Many of the commonly used generative models, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), use likelihood functions to model the posterior distribution. 

These methods use the likelihood function to find the probability density function (PDF) of the data. We can define a function $f_\theta(x)$ as a real valued, unnormalized probabilistic model, and the PDF of the data is given by:

$$p_\theta(x) = \frac{e^{-f_\theta(x)}}{Z_\theta}$$

where 
$$\int p_\theta(x) dx = 1$$ 

and 

$$max_\theta \sum_{i=1}^N \log p_\theta(x_i)$$

The problem with this approach is that the normalizing constant $Z_\theta$ is intractable to compute. This is because the integral of the PDF over the entire space is equal to 1, and the PDF is defined as the ratio of the likelihood function and the normalizing constant. Therefore, the normalizing constant is the integral of the likelihood function over the entire space, which is intractable to compute.

Diffusion models are generative models that use a score function instead of likelihood functions in order to model the posterior distribution that generates the data. The score function is defined as the gradient of the log-likelihood function:

$$s_\theta(x) = \nabla_x \log p_\theta(x) = -\nabla_x f_\theta(x) - \nabla_x log Z_\theta = -\nabla_x f_\theta(x)$$

Since the normalizing constant is not dependent on $x$, its partial derivative with respect to $x$ is zero. Therefore, the score function is equal to the negative gradient of the log-likelihood function, and this solves the problem of the intractable normalizing constant.

In a more visual sense, as seen in the figure below, score-based models learn the gradients of the log-likelihood function, as denoted by the arrows, instead of the contours of the log-likelihood function, as denoted.

![Score-Based Models](/assets/img/blog/2023-04-26-CSDI/Picture1.png)

The method used to train these models is based on the Fisher Divergence between the actual gradients and the gradients of the model. The Fisher Divergence is defined as:

$$E_{p(x)}[||\nabla_x \log p(x) - s_\theta(x)||^2]$$

where $p(x)$ is the true data distribution, and $s_\theta(x)$ is the score function of the model. The Fisher Divergence is minimized when the gradients of the model are equal to the gradients of the true data distribution.

![Fisher Divergence](/assets/img/blog/2023-04-26-CSDI/Picture2.png)

Of course, we do not have access to the true data distribution, so we use the score-matching objective function instead:

$$E_{p_{data(x)}}[tr(\nabla_x s_\theta(x)) + \frac{1}{2} ||s_\theta(x)||^2]$$

where $\nabla_x s_\theta(x)$ is the Jacobian of the score function. However, the Jacobian is hard to compute in deep neural networks, and instead, we use either of the following two methods:
- Use perturbations, as in the Denoising Score Matching, which we will discuss in more detail in this post.
- Use projections (sliced score matching), which are briefly defined as:
  
$$E_{p_v} E_{p{data}} [v^T \nabla_x s_\theta(x) v + \frac{1}{2} ||s_\theta(x)||^2]$$

where $v$ is a distributed by $p_v(v)$, which can be a multivariate standard normal distribution.

## Denoising Score Matching

The key challenge in training score-based models is the fact that we do not have much information on the low-density regions, and therefore, the gradients in these regions are inaccurate. To solve this problem, we may use different levels of noise to perturb the data, which leads us to have a better estimate of the gradients in the low density regions.

![Denoising Score Matching](/assets/img/blog/2023-04-26-CSDI/Picture3.png)

Denoising Score Matching methods follow this intuition and add multiple levels of noise to the data, and then train the model to reverse this noising process.

There is also another aspect of looking at this approach coming from physics, called the Langevin diffusion. 

One of the most popular methods of implementing this approach is Denoising Diffusion Probabilistic Models (DDPM). DDPM consists of two processes, namely, the forward process and the reverse process. The forward process is defined as:

$$q(x_t|x_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$

where $\beta_t$ is the diffusion coefficient, and $I$ is the identity matrix. Intuitively the forward process adds noise in consecutive steps to the data by multiplying the data by $\sqrt{1-\beta_t}$ and adding noise with variance $\beta_t$. Where $\beta_t$ is called a variance schedule and can be simply linear or can be a more complex function of $t$. In the end of the noising process, the data is nearly a standard normal distribution.

The reverse process is defined as:

$$p(x_{t-1}|x_t) = \mathcal{N}(x_{t-1} ; \mu_\theta(x_t, t), \sigma_\theta^2(x_t, t) I)$$

where 

$$\mu_\theta(x_t, t) = \frac{1}{\alpha_t} (x_t - \frac{\beta_t}{\sqrt{1-\alpha_t}} \epsilon_\theta(x_t, t))$$

and 

$$\sigma_\theta^2(x_t, t) = \tilde{\beta_t}^{\frac{1}{2}}$$

where $\hat{\alpha_t} = 1-\beta_t$ and $\alpha_t = \prod_{i=1}^t \hat{\alpha_i}$.

Based on this characterization, we do not actually need to add the noise step by step, and we can get $x_t$ directly from $x_0$ by using the following equation:

$$x_t = \sqrt{\alpha_t} x_0 + \sqrt{1-\alpha_t} \epsilon$$

where $\epsilon \sim \mathcal{N}(0, I)$.

Also, $\tilde{\beta_t}$ is defined as:

$$\tilde{\beta_t} = \frac{1-\alpha_t -1}{1-\alpha_t}$$ 

when $t>1$ and $\beta_1$ when $t=1$.

We can then train the model by minimizing the following loss function:

$$\mathcal{L}(\theta) = min_\theta E_{x_0 \sim q(x_0) , \epsilon \sim \mathcal{N}(0, I)} ||\epsilon - \epsilon_\theta(x_t, t)||^2$$

A more in-depth description of this method can be found in [my friend's blog post](http://127.0.0.1:4000/jekyll/update/2023/04/07/DDPM-paper-learning-note.html).

DDPM is a very powerful method, and it has been shown to be able to generate high-quality images, especially when paired with a language model. Some examples of such models include [Midjourney](midjourney.com) and [DALL-E 2](https://openai.com/product/dall-e-2). Following are some examples of the images generated by Midjourney:

![Photos from Midjourney](/assets/img/blog/2023-04-26-CSDI/midjourney_v4_hero_2-800x450.jpg)

# Time Series Imputation Using Score-Based Models

The CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation paper by [Tashiro et al.](https://arxiv.org/abs/2107.03502) introduces a method of using score-based models for time series imputation. The main idea is to use the diffusion process to impute the missing values in a self-supervised manner while conditioning the diffusion process at each backward step on the observed values without noising these values.

## Problem Definition

The problem of time series imputation is defined as follows:

Given N multivariate time series with missing values, $X = \{x_{1:F, 1:T} \in \mathbb{R}^{F \times T}\}$ where $F$ is the number of features and $T$ is the number of time steps, we want to impute the missing values in $X$. We can think of the missing values as a mask on the data $M = \{m_{1:F, 1:T} \in \{0, 1\}^{F \times T}\}$ where $m_{f, t} = 1$ if $x_{f, t}$ is missing and $m_{f, t} = 0$ otherwise.

## Conditioning on the Observed Values

The main difference between the traditional score-based models and CSDI is that in CSDI, we condition the diffusion process on the observed values. This is done by using the following equation:

$$p(x_{t-1}^{m=1}|x_t^{m=1}, x_{0}^{m=0}) = \mathcal{N}(x_{t-1}^{m=1} ; \mu_\theta(x_t^{m=1}, t | x_{0}^{m=0}), \sigma_\theta(x_t^{m=1}, t | x_{0}^{m=0})I)$$

which adds the conditions to the loss function as follows:

$$\mathcal{L}(\theta) = min_\theta E_{x_0^{m=0} \sim q(x_0^{m=0}) , \epsilon \sim \mathcal{N}(0, I)} ||\epsilon - \epsilon_\theta(x_t^{m=1}, t | x_{0}^{m=0})||^2$$

An implementation of this method in Pytorch looks like this:

```python
    def forward(self, data):

        b, t, f = data.shape

        noise_mask = self.get_mask(data, self.strategy).to(self.device)
        noise = torch.randn((b, t, f)).to(self.device)
        noise = (noise_mask * noise)

        diffusion_t = torch.randint(0, self.diffusion_steps, (b,1)).squeeze(1)
        alpha = self.alpha[diffusion_t].unsqueeze(1).unsqueeze(2).to(self.device)

        noised_data = data * noise_mask
        noised_data = noised_data * (alpha**0.5) + noise * ((1 - alpha)**0.5)
        conditional_data = data * (1 - noise_mask)
        noised_data = noised_data + conditional_data
        noised_data = noised_data.float()

        predicted_noise = self.model(noised_data.unsqueeze(3),
        noise_mask.unsqueeze(3), diffusion_t)

        predicted_noise = predicted_noise * noise_mask

        return (predicted_noise, noise, noise_mask)
```

Where the loss function is defined as follows:

```python
    
    def loss_func(self, predicted_noise, noise, noise_mask):

        residual = noise - predicted_noise
        num_obs = torch.sum(noise_mask)
        loss = (residual**2).sum() / num_obs

        return(loss)
    
```


In this code, we create an alpha value, which is the cumulative product of all alpha hats, as defined above. We then sample a time t randomly from a uniform distribution between 0 and the number of diffusion steps, which is a hyperparameter. We also create a random noise mask (more on this later) and sample some normal noise. We then add the noise to the parts of the data that are masked. These are basically the places where $m_{f, t} = 1$. We then add the non-noised parts of the data to the noised data. We then pass the noised data to the model, which is a transformer-based loop. The model then predicts the noise, which is then compared to the actual noise to calculate the loss in the masked parts of the data.

![CSDI Model](/assets/img/blog/2023-04-26-CSDI/Picture4.png)

## Masking Strategy

To train a model using the above method, we need to create a mask for the data. This mask is used to mask the data at each diffusion step. One of the beauties of this method is that we can use any masking strategy we want. For example, if the intent of the model is to impute missing-at-random values, we can randomly choose to mask some parts of the data. If we have missing-not-at-random values, we can use a more sophisticated masking strategy. The model can also be used for interpolation by masking all the features at a given time step or forecasting by masking all the features at the end of the time series.

The method uses a self-supervised approach, where we mask some parts of the data that are available and condition on the rest of the available data in the training time. 

![Masking Strategy](/assets/img/blog/2023-04-26-CSDI/Picture5.png)

## The Model

Briefly, the paper uses two transformers, one going through the features and one going through the time steps. The model also receives embeddings of time, feature, and diffusion time steps. By using the double transformers, the model is able to understand both temporal and feature-wise dependencies.

![Model Architecture](/assets/img/blog/2023-04-26-CSDI/Picture6.png)

## Results

The paper uses two datasets:

- Healthcare dataset in PhysioNet Challenge 2012 which consists of 4000 clinical time series with 35 variables for 48 hours from intensive care unit (ICU). The paper uses 10/50/90% missingness for the data.
- Air quality dataset: hourly sampled PM2.5 measurements from 36 stations in Beijing for 12 months and set 36 consecutive time steps as one time series.

The paper uses Continuous ranked probability score (CRPS) for probabilistic imputation, which
measures the compatibility of an estimated probability distribution with an observation and is defined as follows:

$$CRPS(F, y) = \int_{-\infty}^{\infty} (F(z) - \mathbb{1}(z \geq y))^2 dz$$

where $F$ is the cumulative distribution function of the predicted distribution and $y$ is the observed value. The paper also uses

And Mean absolute error (MAE) for deterministic imputation.

Overall, the results of the paper are remarkable and the paper does achieve the state-of-the-art in both deterministic and probabilistic imputation in both cases:

![Results](/assets/img/blog/2023-04-26-CSDI/Picture7.png)

![Results](/assets/img/blog/2023-04-26-CSDI/Picture8.png)

# Conclusion

In this blog post, we reviewed the paper "The CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation" by [Tashiro et al.](https://arxiv.org/abs/2107.03502), as well as the diffusion models and the score-based models. Overall, the method is quite powerful and can be used for a variety of tasks, including imputation, forecasting, and interpolation. The method is also quite flexible and can be used with any masking strategy. The paper also achieves state-of-the-art results in both deterministic and probabilistic imputation.

# Future Work

One can think of many extensions of the model, for example, to different downstream tasks, such as classification. One can also explore CSDI on other data modalities, such as audio and video, which have a similar time-based aspect.

# References
- <https://arxiv.org/abs/2107.03502>
- <https://arxiv.org/pdf/1905.07088.pdf>
- <https://yang-song.net/blog/2021/score/>
- <https://arxiv.org/abs/2006.11239>
- <https://arxiv.org/abs/2011.13456>




